Há quase 4 anos, em meu primeiro artigo aqui nesse portal “Máquinas que compreendem a linguagem humana”, escrevi sobre o estrelado sistema Watson/IBM e seu grande feito no mundo das disputas de conhecimento televisas no melhor estilo “Quem que ser um milionário?” lá no início dessa década; não apenas isso, mas principalmente sobre sua enorme capacidade de analisar a linguagem escrita humana a ponto de, por exemplo, processar milhares de artigos científicos sobre uma dada proteína associada a muitos tipos de câncer e identificar sozinho várias outras proteínas relacionadas a esta e que haviam passado desapercebidas pelos pesquisadores e médicos.
Pois bem, essa invejável capacidade cognitiva do Watson se disseminou e evoluiu. Isso é o que mostra um artigo publicado no início deste mês na revista Nature [2]. Pesquisadores do Departamento de Energia da Universidade de Berkeley (Califórnia, EUA) criaram uma máquina inteligente capaz de realizar a “leitura” de mais de 3 milhões de resumos (abstracts) de artigos científicos publicados entre os anos de 1922 e 2018; ou seja, quase 100 anos de pesquisa científica foram analisados pela máquina. Além de descobrir materiais (elementos químicos) com propriedades especiais, ela provou seu alto poder de cognição ao recomendar materiais para aplicações funcionais vários anos antes de quando essa descoberta foi de fato feita pelos humanos. A título de exemplo, através da leitura de trabalhos publicados até o ano de 2009, houveram duas descobertas feitas pela máquina que os humanos só chegaram em 2018.
Isso sugere que o conhecimento latente sobre descobertas futuras está, em grande parte, embutido em publicações já existentes, mas permanece desconhecido, simplesmente porque ninguém ainda vasculhou o suficiente. O fato é que particularmente para a pesquisa em Ciência dos Materiais, a principal fonte de dados interpretáveispor computadores advém de bancos de dados estruturados, dados bem-comportados. O problema é que esse tipo de dado representa uma fração muito pequena de todo o conhecimento presente na literatura científica relacionada e só acessada através da leitura de textos grandes, difíceis e que demandam muito tempo. O que uma máquina inteligente moderna como essa é capaz de fazer, assim como fora o Watson anteriormente, é extrair propriedades relevantes e descobrir conexões e relacionamentos complexos entre elementos de dados presentes no corpo massivo da literatura científica de maneira coletiva.
Bom, a dita cuja foi treinada a partir de um grande conjunto de resumos científicos. Esses resumos foram obtidos a partir de bases científicas relevantes, tais como a Scopus da Elsevier e a Springer Nature. O aspecto central do modelo criado refere-se à representação dos elementos de texto, as palavras. Cada palavra é representada por um vetor multidimensional que preserva seus relacionamentos sintáticos e semânticos com outras palavras, conseguidos através de informação sobre co-ocorrência de palavras nos textos. Na literatura de Processamento de Linguagem Natural (PLN), esse vetor de representação é chamado de word embeddings [3], que são gerados a partir de algoritmos de aprendizado profundo que dispensam qualquer intervenção humana; no caso do domínio desse trabalho, não houve qualquer inserção explícita de conhecimento em Química e, ainda assim, esses embeddings conseguiram capturar conceitos complexos de Ciência dos Materiais como, por exemplo, a estrutura subjacente da tabela periódica e as relações estrutura-propriedade nos materiais. O modelo produziu, por exemplo, uma lista ordenada de materiais que eram fortes candidatos a possuir propriedades termoelétricas (capacidade de converter calor em energia).
Dentre os algoritmos de aprendizado existentes atualmente para tal fim, os autores utilizaram um dos mais famosos e bem-sucedidos, o chamado Word2vec, particularmente a variação Skip-gram [4]. A hipótese central que norteia a técnica, já validada em diferentes trabalhos de PLN, é a de que já que palavras diferentes com significados similares frequentemente aparecem em contextos similares, seus embeddings correspondentes também serão similares. O modelo manipulou os vetores para descobrir termos, conceitos e princípios fundamentais da Ciência dos Materiais. Dois exemplos ilustram essa capacidade de manipulação. Muitas palavras encontradas nos resumos representam composições químicas de materiais e os cinco materiais mais similares ao LiCoO2 (um composto de cátodo de íon-lítio bem conhecido) pode ser determinado através de um produto vetorial (projeção) dos embeddings normalizados. De acordo com o modelo treinado, as composições com a maior similaridade ao LiCoO2 foram LiMn2O4, LiNi0.5Mn1.5O4, LiNi0.8Co0.2O2, LiNi0.8Co0.15Al0.05O2 e LiNiO2 — todas elas também são, não por acaso, materiais de cátodo de íon-lítio. Outro exemplo, diz respeito ao suporte a analogias: ‘NiFe’ está para ‘ferromagnetic’ tal como ‘IrMn’ está para ‘?’. O modelo resolve o problema através de uma operação aritmética no espaço vetorial: ferromagnetic – NiFe + IrMn ≈ …. antiferromagnetic!!! Voilà! 
A mensagem passada pelo Watson e seus descendentes é clara: escolha um campo da Ciência e eu o ajudarei com insights que você provavelmente só terá daqui a uma década, o ajudarei com a cura que demoraria a ser descoberta, com o dinheiro que não seria economizado, com o veículo que jamais o transportaria, com a estrela que só seria notada quando não mais existisse, com a tecnologia que só “compraria” sua segurança, sua saúde, sua paz, daqui a muitos e muitos anos. Se o conhecimento existe desde ontem, “hoje” já é muito longe!
Se existe uma obra artística impossível de tirar da cabeça é a da pintura surrealista “La persistència de la memòria” (1931) do mestre catalão Salvador Domingo Felipe Jacinto Dali i Domènech. Sabe aquela com os relógios derretidos e uma paisagem com mar no horizonte? Tenho ela na mente desde garoto. Um dia fui procurar pela explicação da obra nas palavras do próprio Dalí: “Toda a minha ambição no campo pictórico é materializar as imagens da irracionalidade concreta com a mais imperialista fúria da precisão”. Ah, tá! :/
Agora, imaginem o próprio Dalí e todo o seu jeito bem peculiar de ser e falar, pronunciando essa frase… Na verdade, nem precisamos mais imaginar, podemos vê-lo e ouvi-lo, ao vivo, no Museu Dalí (St. Petersburg, Florida, E.U.A.), pronunciando essa e outras frases bem mais contemporâneas e interagindo conosco; o cara usa até smartphone para tirar uma selfie com os visitantes. Tudo isso é possível graças a uma iniciativa da empresa Goodby Silverstein & Partners of San Francisco (GS&P), que aplicou de forma magistral a tecnologia do Deep Fake à figura de Dalí especialmente para o museu em questão [2] (já escrevi mais detalhadamente sobre o Deep Fake no artigo “A impressionante habilidade de se criar vídeos falsos artificialmente”; confiram lá!). A equipe de desenvolvedores treinou uma rede neuronal convolucional a partir de mais de 6 mil quadros de imagens históricas de Dalí. A Inteligência Artificial treinada aprendeu cada aspecto do rosto e feições do pintor e as sobrepôs no corpo de um ator com características físicas gerais semelhantes. Para fazê-lo falar com autenticidade e desenvoltura sobre temas atuais, sua personalidade foi resgatada a partir de citações extraídas de velhas entrevistas, livros e cartas pessoais. Todo o processo de treinamento do modelo exigiu mais de mil horas de aprendizado de máquina. O resultado são mais de 190 mil diferentes possibilidades de interação com os visitantes; interações essas que chegam a mudar a depender do dia e de aspectos como o clima. No link da citação anterior é possível ver um vídeo do resultado do trabalho. Fantástico!
Quase no mesmo momento em que tomei conhecimento desse trabalho, me deparei com um outro recentemente desenvolvido pelo grupo de pesquisa da OpenAI (Califórnia/E.U.A.): trata-se de um novo algoritmo de aprendizagem profunda, denominado de MuseNet, que consegue gerar automaticamente músicas originais [3]. A coisa é capaz de criar novas composições musicais de 4 minutos de duração fazendo uso de até 10 instrumentos diferentes e combinando diferentes estilos musicais. De acordo com um estilo musical e conjunto de instrumentos desejados pelo usuário, a IA gera a nova música, porém, respeitando o perfil de um determinado compositor. É como se Beethoven acordasse no século 21 e, mantendo sua assinatura musical, criasse uma nova sinfonia numa pegada pop à lá Bon Jovi com uso de bateria e guitarra, ou, o Mozart modificasse progressões de acordes, instrumentos e aplicasse seu estilo à canção “Poker Face” de Lady Gaga, utilizada como ponto de partida.  A MuseNet consiste de uma rede neuronal de 72 camadas que foi treinada com os kernels recomendados e já otimizados da rede neural profunda Sparse Transformer [4], que foi concebida para ser capaz de prever o que virá em seguida em uma sequência de dados sejam eles do tipo texto, áudio ou imagem. Centenas de milhares de arquivos MIDI de variadas fontes foram coletados para o treinamento da rede que foi ensinada a descobrir padrões de harmonia, ritmo e estilo musical.
Não é preciso muito esforço para perceber as possibilidades maravilhosas da junção dos dois trabalhos acima, certo? Ou sou só eu que gostaria de poder assistir “ao vivo” a uma nova performance de Tom e Elis ou dos Beatles ou de Chopin, com música boa e …original?!
Em seu último pronunciamento público em janeiro de 1989, Dalí disse: “Quando você é um gênio, você não tem o direito de morrer.”. De fato. Sendo assim, diante da possibilidade de corrigirmos o curso das coisas, “ressuscitando” alguns dos maiores gênios que a humanidade já produziu e que tanto nos inspiram ou emocionam, ainda que sob a ameaça concreta da proliferação de montagens sedentas de desgraça alheia e caos, criadas e motivadas por loucos, prefiro que encaremos de frente o risco dessas tecnologias por acreditar que sobre “genialidade” e “loucura”, apesar de bem tênue a linha, somos bem mais numerosos no primeiro lado dela.
